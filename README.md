## Modern ELT Data Pipeline with Airbyte, Snowflake, and dbt

Built an end-to-end ELT pipeline that ingests data with **Airbyte**, loads it into **Snowflake**, and transforms it with **dbt** to create staging views and fact tables for analysis. This project demonstrates modern data engineering practices for data integration and analytics.

## What I Did
- Connected multiple data sources: Google Sheets survey, CSV files, and Snowflake Marketplace datasets.  
- Configured Airbyte to load raw data into Snowflake.  
- Created dbt models to clean and transform data (e.g., survey transformations, valid ticker staging, trading fact table).  
- Analyzed data in Snowflake and Python (`pandas`, `matplotlib`) to calculate profit metrics and build visualizations.  

## Tech Stack
Airbyte Â· Snowflake Â· dbt Â· SQL Â· Python  

## ðŸ“‚ Repo Structure
```p4-elt-pipeline/
â”œâ”€ p4.ipynb        # SQL + Python analysis
â”œâ”€ report.pdf      # Final report with screenshots
â”œâ”€ files/          # CSV exports from dbt models
â”‚  â”œâ”€ staging_valid_fx_tickers.csv
â”‚  â”œâ”€ staging_valid_stock_tickers.csv
â”‚  â”œâ”€ staging_valid_stock_info.csv
â”‚  â”œâ”€ staging_valid_fx_info.csv
â”‚  â””â”€ fact_tab_trading.csv
â””â”€ models/         # dbt transformations
   â”œâ”€ schema.yml
   â”œâ”€ staging/
   â”‚  â”œâ”€ transform_survey.sql
   â”‚  â”œâ”€ staging_valid_fx_tickers.sql
   â”‚  â”œâ”€ staging_valid_stock_tickers.sql
   â”‚  â”œâ”€ staging_valid_stock_info.sql
   â”‚  â””â”€ staging_valid_fx_info.sql
   â””â”€ marts/
      â””â”€ fact_tab_trading.sql
```

